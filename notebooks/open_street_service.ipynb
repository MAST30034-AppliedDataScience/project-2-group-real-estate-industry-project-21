{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Street Service Notebook\n",
    "\n",
    "Coded up with help of Chatgpt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to encode the info\n",
    "\n",
    "How are we going to use this data? \n",
    "\n",
    "searching for places within the 1km radius of the house\n",
    "\n",
    "Need to be able to get the coords of the place so can work out distance + route(?) by car to both train station and CBD + \n",
    "- Train stations \n",
    "- CBD\n",
    "- parks\n",
    "- hosptials\n",
    "- supermarkets\n",
    "- schools\n",
    "- shopping districts\n",
    "\n",
    "\n",
    "\n",
    "- get a rough estimate for the amount of each within the 1km radius (just a count)\n",
    "- Also only print off/ Store the information of the places with either the coords or the address... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melbourne CBD Coords (LAT, LONG)\n",
    "\n",
    "coords_cbd = [-37.8124, 144.9623]\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Functions\n",
    "\n",
    "1.the get_coordinates() outputs the coordinates for every input of address\n",
    "\n",
    "2.the find_nearby_locations outputs the nearby(1km) amentities for every input of address coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "import requests\n",
    "import folium\n",
    "\n",
    "def get_coordinates(address):\n",
    "    geolocator = Nominatim(user_agent=\"myGeocoder\")\n",
    "    location = geolocator.geocode(address)\n",
    "    return (location.latitude, location.longitude)\n",
    "\n",
    "\n",
    "def find_nearby_locations(lat, lon, location_type):\n",
    "    tags = {\n",
    "        \"schools\": '[\"amenity\"=\"school\"]',\n",
    "        \"parks\": '[\"leisure\"=\"park\"]',\n",
    "        \"supermarkets\": '[\"shop\"=\"supermarket\"]',\n",
    "        \"shopping_districts\": '[\"shop\"=\"mall\"]',\n",
    "        \"train_stations\": '[\"railway\"=\"station\"]',\n",
    "        \"hospitals\": '[\"amenity\"=\"hospital\"]'\n",
    "    }\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    [out:json];\n",
    "    (\n",
    "      node{tags[location_type]}(around:1000,{lat},{lon});\n",
    "      way{tags[location_type]}(around:1000,{lat},{lon});\n",
    "      relation{tags[location_type]}(around:1000,{lat},{lon});\n",
    "    );\n",
    "    out body;\n",
    "    \"\"\"\n",
    "    response = requests.get('http://overpass-api.de/api/interpreter', params={'data': query})\n",
    "    data = response.json()\n",
    "    return data['elements']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process each Location\n",
    "\n",
    "Note that has the assumption that the address of the places found are within the suburb and postcode \n",
    "\n",
    "This shows amenities address as much as it can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def process_locations(locations, address):\n",
    "    \n",
    "    pattern = r\",\\s*([A-Za-z\\s]+)\\s*VIC\\s*(\\d{4})\"\n",
    "    match = re.search(pattern, address)\n",
    "    \n",
    "    if match:\n",
    "        suburb = match.group(1).strip()  # Suburb (e.g., Coburg)\n",
    "        postcode = match.group(2).strip()  # Postcode (e.g., 3058)\n",
    "    \n",
    "    # Dictionary to store valid locations\n",
    "    valid_locations = []\n",
    "    \n",
    "    for loc in locations:\n",
    "        \n",
    "        if 'tags' in loc:\n",
    "            name = loc['tags'].get('name')\n",
    "            location_lat = loc.get('lat')\n",
    "            location_lon = loc.get('lon')\n",
    "\n",
    "            # Get the address information from tags if available\n",
    "            \n",
    "            street_number = loc['tags'].get('addr:housenumber')\n",
    "            street = loc['tags'].get('addr:street')\n",
    "            postcode = loc['tags'].get('addr:postcode', postcode)\n",
    "            suburb = loc['tags'].get('addr:suburb', suburb)\n",
    "            # Construct the address\n",
    "            address = f\"{street_number} {street}, {suburb}, {postcode}\" if street_number and street else None\n",
    "            \n",
    "            # Filter locations with either coordinates or address\n",
    "            if (location_lat and location_lon) or address:\n",
    "                valid_locations.append({\n",
    "                    'name': name or \"Unnamed Location\",\n",
    "                    'lat': location_lat,\n",
    "                    'lon': location_lon,\n",
    "                    'address': address\n",
    "                })\n",
    "    \n",
    "    return valid_locations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random sampling \n",
    "\n",
    "max 500 properties from every suburb in the datasets we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m6/nbs95xhj6r39965k82b053ph0000gn/T/ipykernel_49464/3436795265.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Postcodes', group_keys=False).apply(lambda x: x.sample(min(len(x), 5)))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Open the CSV file\n",
    "df = pd.read_csv('properties_and_coordinates.csv')\n",
    "\n",
    "# Extract postcodes\n",
    "df.insert(0, 'Postcodes', df['Address'].apply(lambda x: x.split('VIC')[-1].strip()))\n",
    "\n",
    "# Randomly select max 500 rows from each postcodes and build a dataframe \n",
    "df = df.groupby('Postcodes', group_keys=False).apply(lambda x: x.sample(min(len(x), 5)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get nearby amenitites for each property \n",
    "\n",
    "Using the randomly sampled dataframe and previously built function\n",
    "\n",
    "Output the coordinates for every found anmentity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "summary_list = []\n",
    "\n",
    "# Build the filters of amenities\n",
    "location_types = [\"schools\", \"parks\", \"supermarkets\", \"shopping_districts\", \"train_stations\", \"hospitals\"]\n",
    "\n",
    "# Create an empty DataFrame with summary columns\n",
    "summary_df = pd.DataFrame(columns=['Postcodes', 'Address', 'URLS', 'Latitude', 'Longitude', 'Location Type', 'Count', 'Location Name', 'Location Address', 'Location Latitude', 'Location Longitude'])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    postcodes = row['Postcodes']\n",
    "    address = row['Address']\n",
    "    urls = row['URLS']\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "\n",
    "    summary = {}\n",
    "\n",
    "    try:\n",
    "        for location_type in location_types:\n",
    "            try:\n",
    "                locations = find_nearby_locations(lat, lon, location_type)\n",
    "                \n",
    "                # Check if locations are found and process them\n",
    "                if locations:\n",
    "                    valid_locations = process_locations(locations, address)\n",
    "                    summary[location_type] = {\n",
    "                        'count': len(locations),  \n",
    "                        'locations': valid_locations  \n",
    "                    }\n",
    "                else:\n",
    "                    print(f\"No locations found for {location_type} at {address}\")\n",
    "            \n",
    "            # Catch potential JSON errors or other exceptions within find_nearby_locations\n",
    "            except JSONDecodeError as e:\n",
    "                print(f\"JSON decoding error for {address} and location type {location_type}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Add a delay of 1 second after processing each location_type\n",
    "            time.sleep(1)\n",
    "\n",
    "    except UnboundLocalError as e:\n",
    "        print(f\"Error processing {address}: {e}\")\n",
    "        continue  # Skip to the next iteration if there's an error\n",
    "\n",
    "    # Add CBD and its summary as an individual amenity for every property\n",
    "    summary_df = pd.concat([summary_df, pd.DataFrame([{\n",
    "        'Postcodes': postcodes,\n",
    "        'Address': address,\n",
    "        'URLS': urls,\n",
    "        'Latitude': lat,\n",
    "        'Longitude': lon,\n",
    "        'Location Type': 'CBD',  # Example row\n",
    "        'Count': '1',\n",
    "        'Location Name': 'CBD',\n",
    "        'Location Address': '',\n",
    "        'Location Latitude': '-37.8124',\n",
    "        'Location Longitude': '144.9623'\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "    # Add the summary of found nearby amenities for every property \n",
    "    for location_type, counts in summary.items():\n",
    "        count = counts['count']\n",
    "        for loc in counts['locations']:\n",
    "            summary_df = pd.concat([summary_df, pd.DataFrame([{\n",
    "                'Postcodes': postcodes,\n",
    "                'Address': address,\n",
    "                'URLS': urls,\n",
    "                'Latitude': lat,\n",
    "                'Longitude': lon,\n",
    "                'Location Type': location_type,\n",
    "                'Count': count,\n",
    "                'Location Name': loc['name'],\n",
    "                'Location Address': loc['address'],\n",
    "                'Location Latitude': loc['lat'],\n",
    "                'Location Longitude': loc['lon']\n",
    "            }])], ignore_index=True)\n",
    "\n",
    "    # Add a delay of 1 second between processing each property\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix\n",
    "For amentities missing the corrdinates,using the geopy to fix\n",
    "\n",
    "But their are still some minor errors\n",
    "\n",
    "Output a fixed properties_stats.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "\n",
    "# Initialize the geolocator\n",
    "geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "\n",
    "\n",
    "# Build another Function to get the latitude and longitude from an address,and handle the errors\n",
    "def get_lat_lon(address):\n",
    "    try:\n",
    "        location = geolocator.geocode(address)\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "# Iterate through the rows and fill missing lat/lon\n",
    "for idx, row in summary_df.iterrows():\n",
    "    if pd.isnull(row['Location Latitude']) or pd.isnull(row['Location Longitude']):\n",
    "        lat, lon = get_lat_lon(row['Location Address'])\n",
    "        summary_df.at[idx, 'Location Latitude'] = lat\n",
    "        summary_df.at[idx, 'Location Longitude'] = lon\n",
    "        time.sleep(4)  # Sleep to avoid overwhelming the API\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "summary_df.to_csv('properties_stats.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
