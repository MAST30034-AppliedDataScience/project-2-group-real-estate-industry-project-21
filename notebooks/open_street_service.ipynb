{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Street Service Notebook\n",
    "\n",
    "Coded up with help of Chatgpt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to encode the info\n",
    "\n",
    "How are we going to use this data? \n",
    "\n",
    "searching for places within the 1km radius of the house\n",
    "\n",
    "Need to be able to get the coords of the place so can work out distance + route(?) by car to both train station and CBD + \n",
    "- Train stations \n",
    "- CBD\n",
    "- parks\n",
    "- hosptials\n",
    "- supermarkets\n",
    "- schools\n",
    "- shopping districts\n",
    "\n",
    "\n",
    "\n",
    "- get a rough estimate for the amount of each within the 1km radius (just a count)\n",
    "- Also only print off/ Store the information of the places with either the coords or the address... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melbourne CBD Coords (LAT, LONG)\n",
    "\n",
    "coords_cbd = [-37.8124, 144.9623]\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Functions\n",
    "\n",
    "1.the get_coordinates() outputs the coordinates for every input of address\n",
    "\n",
    "2.the find_nearby_locations outputs the nearby(1km) amentities for every input of address coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "import requests\n",
    "import folium\n",
    "\n",
    "def get_coordinates(address):\n",
    "    geolocator = Nominatim(user_agent=\"myGeocoder\")\n",
    "    location = geolocator.geocode(address)\n",
    "    return (location.latitude, location.longitude)\n",
    "\n",
    "\n",
    "def find_nearby_locations(lat, lon, location_type):\n",
    "    tags = {\n",
    "        \"schools\": '[\"amenity\"=\"school\"]',\n",
    "        \"parks\": '[\"leisure\"=\"park\"]',\n",
    "        \"supermarkets\": '[\"shop\"=\"supermarket\"]',\n",
    "        \"shopping_districts\": '[\"shop\"=\"mall\"]',\n",
    "        \"train_stations\": '[\"railway\"=\"station\"]',\n",
    "        \"hospitals\": '[\"amenity\"=\"hospital\"]'\n",
    "    }\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    [out:json];\n",
    "    (\n",
    "      node{tags[location_type]}(around:1000,{lat},{lon});\n",
    "      way{tags[location_type]}(around:1000,{lat},{lon});\n",
    "      relation{tags[location_type]}(around:1000,{lat},{lon});\n",
    "    );\n",
    "    out body;\n",
    "    \"\"\"\n",
    "    response = requests.get('http://overpass-api.de/api/interpreter', params={'data': query})\n",
    "    data = response.json()\n",
    "    return data['elements']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process each Location\n",
    "\n",
    "Note that has the assumption that the address of the places found are within the suburb and postcode \n",
    "\n",
    "This shows amenities address as much as it can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def process_locations(locations, address):\n",
    "    \n",
    "    pattern = r\",\\s*([A-Za-z\\s]+)\\s*VIC\\s*(\\d{4})\"\n",
    "    match = re.search(pattern, address)\n",
    "    \n",
    "    if match:\n",
    "        suburb = match.group(1).strip()  # Suburb (e.g., Coburg)\n",
    "        postcode = match.group(2).strip()  # Postcode (e.g., 3058)\n",
    "    \n",
    "    # Dictionary to store valid locations\n",
    "    valid_locations = []\n",
    "    \n",
    "    for loc in locations:\n",
    "        \n",
    "        if 'tags' in loc:\n",
    "            name = loc['tags'].get('name')\n",
    "            location_lat = loc.get('lat')\n",
    "            location_lon = loc.get('lon')\n",
    "\n",
    "            # Get the address information from tags if available\n",
    "            \n",
    "            street_number = loc['tags'].get('addr:housenumber')\n",
    "            street = loc['tags'].get('addr:street')\n",
    "            postcode = loc['tags'].get('addr:postcode', postcode)\n",
    "            suburb = loc['tags'].get('addr:suburb', suburb)\n",
    "            # Construct the address\n",
    "            address = f\"{street_number} {street}, {suburb}, {postcode}\" if street_number and street else None\n",
    "            \n",
    "            # Filter locations with either coordinates or address\n",
    "            if (location_lat and location_lon) or address:\n",
    "                valid_locations.append({\n",
    "                    'name': name or \"Unnamed Location\",\n",
    "                    'lat': location_lat,\n",
    "                    'lon': location_lon,\n",
    "                    'address': address\n",
    "                })\n",
    "    \n",
    "    return valid_locations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random sampling \n",
    "\n",
    "max 500 properties from every suburb in the datasets we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/landing/unfixed_3046_3055_properties.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Open the CSV file ( Chnage this with the full dataset?)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/landing/unfixed_3046_3055_properties.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Extract postcodes\u001b[39;00m\n\u001b[1;32m      9\u001b[0m df\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPostcodes\u001b[39m\u001b[38;5;124m'\u001b[39m, df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAddress\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVIC\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/landing/unfixed_3046_3055_properties.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Open the CSV file ( Chnage this with the full dataset?)\n",
    "file_path = './data/landing/unfixed_3046_3055_properties.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract postcodes\n",
    "df.insert(0, 'Postcodes', df['Address'].apply(lambda x: x.split('VIC')[-1].strip()))\n",
    "\n",
    "# Randomly select max 500 rows from each postcodes and build a dataframe \n",
    "df = df.groupby('Postcodes', group_keys=False).apply(lambda x: x.sample(min(len(x), 5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get nearby amenitites for each property \n",
    "\n",
    "Using the randomly sampled dataframe and previously built function\n",
    "\n",
    "Output the coordinates for every found anmentity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "summary_list = []\n",
    "\n",
    "# Build the filters of amenities\n",
    "location_types = [\"schools\", \"parks\", \"supermarkets\", \"shopping_districts\", \"train_stations\", \"hospitals\"]\n",
    "\n",
    "# Create an empty DataFrame with summary columns\n",
    "summary_df = pd.DataFrame(columns=['Postcodes', 'Address', 'URLS', 'Latitude', 'Longitude', 'Location Type', 'Count', 'Location Name', 'Location Address', 'Location Latitude', 'Location Longitude'])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    postcodes = row['Postcodes']\n",
    "    address = row['Address']\n",
    "    urls = row['URLS']\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "\n",
    "    summary = {}\n",
    "\n",
    "    try:\n",
    "        for location_type in location_types:\n",
    "            try:\n",
    "                locations = find_nearby_locations(lat, lon, location_type)\n",
    "                \n",
    "                # Check if locations are found and process them\n",
    "                if locations:\n",
    "                    valid_locations = process_locations(locations, address)\n",
    "                    summary[location_type] = {\n",
    "                        'count': len(locations),  \n",
    "                        'locations': valid_locations  \n",
    "                    }\n",
    "                else:\n",
    "                    print(f\"No locations found for {location_type} at {address}\")\n",
    "            \n",
    "            # Catch potential JSON errors or other exceptions within find_nearby_locations\n",
    "            except JSONDecodeError as e:\n",
    "                print(f\"JSON decoding error for {address} and location type {location_type}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Add a delay of 1 second after processing each location_type\n",
    "            time.sleep(1)\n",
    "\n",
    "    except UnboundLocalError as e:\n",
    "        print(f\"Error processing {address}: {e}\")\n",
    "        continue  # Skip to the next iteration if there's an error\n",
    "\n",
    "    # Add CBD and its summary as an individual amenity for every property\n",
    "    summary_df = pd.concat([summary_df, pd.DataFrame([{\n",
    "        'Postcodes': postcodes,\n",
    "        'Address': address,\n",
    "        'URLS': urls,\n",
    "        'Latitude': lat,\n",
    "        'Longitude': lon,\n",
    "        'Location Type': 'CBD',  # Example row\n",
    "        'Count': '1',\n",
    "        'Location Name': 'CBD',\n",
    "        'Location Address': '',\n",
    "        'Location Latitude': '-37.8124',\n",
    "        'Location Longitude': '144.9623'\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "    # Add the summary of found nearby amenities for every property \n",
    "    for location_type, counts in summary.items():\n",
    "        count = counts['count']\n",
    "        for loc in counts['locations']:\n",
    "            summary_df = pd.concat([summary_df, pd.DataFrame([{\n",
    "                'Postcodes': postcodes,\n",
    "                'Address': address,\n",
    "                'URLS': urls,\n",
    "                'Latitude': lat,\n",
    "                'Longitude': lon,\n",
    "                'Location Type': location_type,\n",
    "                'Count': count,\n",
    "                'Location Name': loc['name'],\n",
    "                'Location Address': loc['address'],\n",
    "                'Location Latitude': loc['lat'],\n",
    "                'Location Longitude': loc['lon']\n",
    "            }])], ignore_index=True)\n",
    "\n",
    "    # Add a delay of 1 second between processing each property\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix\n",
    "For amentities missing the corrdinates,using the geopy to fix\n",
    "\n",
    "But their are still some minor errors\n",
    "\n",
    "Output a fixed properties_stats.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "\n",
    "# Initialize the geolocator\n",
    "geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "\n",
    "\n",
    "# Build another Function to get the latitude and longitude from an address,and handle the errors\n",
    "def get_lat_lon(address):\n",
    "    try:\n",
    "        location = geolocator.geocode(address)\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "# Iterate through the rows and fill missing lat/lon\n",
    "for idx, row in summary_df.iterrows():\n",
    "    if pd.isnull(row['Location Latitude']) or pd.isnull(row['Location Longitude']):\n",
    "        lat, lon = get_lat_lon(row['Location Address'])\n",
    "        summary_df.at[idx, 'Location Latitude'] = lat\n",
    "        summary_df.at[idx, 'Location Longitude'] = lon\n",
    "        time.sleep(4)  # Sleep to avoid overwhelming the API\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "summary_df.to_csv('properties_stats.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
